{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Fine Food Reviews Analysis\n",
    "\n",
    "Data Source: https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in our data (525814, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con=sqlite3.connect('database.sqlite')\n",
    "filtered_data=pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score !=3\"\"\",con)\n",
    "def partition(x):\n",
    "    if x<3:\n",
    "        return 0\n",
    "    return 1\n",
    "actualScore=filtered_data['Score']\n",
    "positiveNegative=actualScore.map(partition)\n",
    "filtered_data['Score']=positiveNegative\n",
    "print(\"Number of data points in our data\", filtered_data.shape)\n",
    "filtered_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exploratory Data Analysis\n",
    "\n",
    "## Data Cleaning: Deduplication\n",
    "\n",
    "Removing Duplicates::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display=pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score !=3 AND UserId=\"AR5J8UI46CURR\" ORDER BY ProductId\"\"\",con)\n",
    "display.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here many rows are duplicated speacially summary part\n",
    "<br>\n",
    "Below code drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data=filtered_data.sort_values('ProductId',axis=0,ascending=True,inplace=False,kind='quicksort',na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep='first',inplace=False)#removing duplicates\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.25890143662969"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking to see how much % of data still remains\n",
    "(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation:-</b> It was also seen that in two rows given below the value of HelpfulnessNumerator is greater than HelpfulnessDenominator which is not practically possible hence these two rows too are removed from calcualtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId              ProfileName  \\\n",
       "0  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "1  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     3                       1      5  1224892800   \n",
       "1                     3                       2      4  1212883200   \n",
       "\n",
       "                                        Summary  \\\n",
       "0             Bought This for My Son at College   \n",
       "1  Pure cocoa taste with crunchy almonds inside   \n",
       "\n",
       "                                                Text  \n",
       "0  My son loves spaghetti so I didn't hesitate or...  \n",
       "1  It was almost a 'love at first bite' - the per...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display= pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3 AND HelpfulnessNumerator>HelpfulnessDenominator\n",
    "ORDER BY ProductID\n",
    "\"\"\", con)\n",
    "\n",
    "display.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    307061\n",
       "0     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(final.shape)\n",
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing: Stemming, stop-word removal and Lemmatization\n",
    "We also need to remove some unnecessary tags and words: <br>\n",
    "Remove html tags<br>\n",
    "Remove any punctuations or limited set of special characters like , or . or # etc.<br>\n",
    "Check if the word is made up of english letters and is not alpha-numeric<br>\n",
    "Check if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)<br>\n",
    "Convert the word to lowercase<br>\n",
    "Remove Stopwords<br>\n",
    "Finally Snowball Stemming the word<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7th review\n",
      "I set aside at least an hour each day to read to my son (3 y/o). At this point, I consider myself a connoisseur of children's books and this is one of the best. Santa Clause put this under the tree. Since then, we've read it perpetually and he loves it.<br /><br />First, this book taught him the months of the year.<br /><br />Second, it's a pleasure to read. Well suited to 1.5 y/o old to 4+.<br /><br />Very few children's books are worth owning. Most should be borrowed from the library. This book, however, deserves a permanent spot on your shelf. Sendak's best.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "i=0;\n",
    "for sent in final['Text'].values:#.values considers values only not labels\n",
    "    if(len(re.findall('<.*?>',sent))):\n",
    "        print(\"{0}th review\".format(i+1))\n",
    "        print(sent)\n",
    "        break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'couldn', 'above', \"haven't\", 'on', 'few', 'you', 'had', 'under', 'why', 'same', \"you'll\", 'mustn', 's', 'or', 'ourselves', \"weren't\", 'through', 'that', 'doing', 'm', 'by', 'because', 'down', 'can', 'up', 'a', 'whom', 'against', \"you're\", 'hadn', 'just', 'but', 'more', \"hadn't\", \"you'd\", \"wouldn't\", \"you've\", 'am', 'between', \"shouldn't\", 'being', \"mightn't\", 'mightn', 'i', 'we', 'itself', 'his', 'its', 'doesn', 'and', 'into', 'until', 'my', 'didn', 'it', 'very', 'me', 'will', 'ma', 'wouldn', 'of', 'if', 'such', 'out', \"don't\", 'during', 'our', 'some', \"won't\", 'did', 'how', \"that'll\", 'he', \"shan't\", 'an', 'don', 'wasn', 'no', 'your', 'where', 're', 'other', 'so', 'were', 'once', \"hasn't\", 'when', 'any', 'this', 'all', 'hasn', 'then', 'won', 't', \"should've\", 'her', 'has', 'is', 'those', 'ain', 'only', 'y', 'haven', 'with', 'about', 'too', \"needn't\", 'having', 'isn', 'be', 'hers', 'yourselves', 'further', \"isn't\", 'needn', 'they', 'weren', 'than', 'these', 'there', 'o', 'not', 'after', 'from', 'herself', 'himself', 'most', 'for', 'yours', 'as', 'ours', 'below', 'was', 'are', 'here', \"couldn't\", 'now', \"aren't\", 'which', 'in', \"doesn't\", 'the', 'shouldn', 'themselves', 'him', 'should', 'aren', 'own', 'at', 'do', \"she's\", 'have', 'again', 'she', 'both', 'each', 'myself', 'to', 'd', 'yourself', 'who', 'nor', \"mustn't\", 've', 'them', 'over', \"wasn't\", 'theirs', \"it's\", 'while', 'before', 'll', \"didn't\", 'off', 'what', 'been', 'does', 'their', 'shan'}\n",
      "************************************\n",
      "tasti\n"
     ]
    }
   ],
   "source": [
    "stop=set(stopwords.words('english'))\n",
    "sno=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "def cleanhtml(sentence):\n",
    "    cleanr=re.compile('<.*?>')\n",
    "    cleantext=re.sub(cleanr,' ',sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence):\n",
    "    cleaned=re.sub(r'[?|!|\\'|#]',r'',sentence)\n",
    "    cleaned=re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned\n",
    "print(stop)\n",
    "print('************************************')\n",
    "print(sno.stem('tasty'))#here tasty and related words to this are stemmed/stored as tasti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 364171/364171 [14:17<00:00, 424.93it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('final.sqlite'): \n",
    "    final_string=[]\n",
    "    all_positive_words=[]\n",
    "    all_negative_words=[]\n",
    "    for i,sent in enumerate(tqdm(final['Text'].values)):\n",
    "        filtered_sentence=[]\n",
    "        sent=cleanhtml(sent)#cleans html tag function created in previous cell\n",
    "        for w in sent.split():\n",
    "            for cleaned_words in cleanpunc(w).split():#here punctuations are also get cleaned so extra space is created\n",
    "                if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):\n",
    "                    if(cleaned_words.lower() not in stop):\n",
    "                        s=(sno.stem(cleaned_words.lower())).encode(\"utf-8\")\n",
    "                        filtered_sentence.append(s)\n",
    "                        if(final['Score'].values)[i]==1:#collecting +ve reviews \n",
    "                            all_positive_words.append(s)\n",
    "                        if(final['Score'].values)[i]==0:#collecting -ve reviews\n",
    "                            all_negative_words.append(s)\n",
    "        str1=b\" \".join(filtered_sentence)\n",
    "        final_string.append(str1)\n",
    "        \n",
    "final['CleanedText']=final_string\n",
    "final['CleanedText']=final['CleanedText'].str.decode(\"utf-8\")\n",
    "conn=sqlite3.connect('final.sqlite')\n",
    "c=conn.cursor()\n",
    "conn.text_factory=str\n",
    "final.to_sql('Reviews',conn,schema=None,if_exists='replace',index=True,index_label=None,chunksize=None,dtype=None)\n",
    "conn.close()\n",
    "\n",
    "with open('positive_words.pkl','wb') as f:\n",
    "    pickle.dump(all_positive_words,f)\n",
    "with open('negative_words.pkl','wb') as f:\n",
    "    pickle.dump(all_negative_words,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('final.sqlite'):\n",
    "    conn = sqlite3.connect('final.sqlite')\n",
    "    final = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", conn)\n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"Please run the above cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (364171, 70740)\n",
      "the number of unique words  70740\n"
     ]
    }
   ],
   "source": [
    "count_vect=CountVectorizer()\n",
    "final_counts=count_vect.fit_transform(final['CleanedText'].values)\n",
    "print(\"the type of count vectorizer \",type(final_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_counts.get_shape())\n",
    "print(\"the number of unique words \", final_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Grams and n-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Positive Words :  [(b'like', 139345), (b'tast', 128766), (b'good', 112234), (b'flavor', 109085), (b'love', 107235), (b'use', 103814), (b'great', 103777), (b'one', 96561), (b'product', 90941), (b'tri', 86722), (b'tea', 83367), (b'coffe', 78440), (b'make', 75049), (b'get', 72048), (b'food', 64330), (b'would', 55556), (b'time', 55166), (b'buy', 54146), (b'realli', 52681), (b'eat', 51873)]\n",
      "Most Common Negative Words :  [(b'tast', 34450), (b'like', 32303), (b'product', 28111), (b'one', 20536), (b'flavor', 19234), (b'would', 17969), (b'tri', 17742), (b'use', 15264), (b'good', 14920), (b'coffe', 14583), (b'get', 13774), (b'buy', 13734), (b'order', 12855), (b'food', 12621), (b'dont', 11855), (b'tea', 11546), (b'even', 11079), (b'box', 10816), (b'amazon', 10060), (b'make', 9835)]\n"
     ]
    }
   ],
   "source": [
    "with open('positive_words.pkl', 'rb') as f:\n",
    "    all_positive_words = pickle.load(f)\n",
    "with open('negative_words.pkl', 'rb') as f:\n",
    "    all_negative_words = pickle.load(f)\n",
    "    \n",
    "freq_dist_positive=nltk.FreqDist(all_positive_words)\n",
    "freq_dist_negative=nltk.FreqDist(all_negative_words)\n",
    "print(\"Most Common Positive Words : \",freq_dist_positive.most_common(20))\n",
    "print(\"Most Common Negative Words : \",freq_dist_negative.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation:-</b> From the above it can be seen that the most common positive and the negative words overlap for eg. 'like' could be used as 'not like' etc.<br> \n",
    "So, it is a good idea to consider pairs of consequent words (bi-grams) or q sequnce of n consecutive words (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (364171, 2902276)\n",
      "the number of unique words including both unigrams and bigrams  2902276\n"
     ]
    }
   ],
   "source": [
    "count_vect=CountVectorizer(ngram_range=(1,2))\n",
    "final_bigram_counts=count_vect.fit_transform(final['CleanedText'].values)\n",
    "print(\"the type of count vectorizer \",type(final_bigram_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_bigram_counts.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_bigram_counts.get_shape()[1])#on increasing grams dimension increases, that's why as compared to previous results of 115k here results gone to 2.9 million"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer  (364171, 2902276)\n",
      "the number of unique words including both unigrams and bigrams  2902276\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect=TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf=tf_idf_vect.fit_transform(final['CleanedText'].values)\n",
    "print(\"the type of count vectorizer \",type(final_tf_idf))\n",
    "print(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some sample features(unique words in the corpus) ['anxious order', 'anxious pill', 'anxious pop', 'anxious probabl', 'anxious product', 'anxious puls', 'anxious purchas', 'anxious read', 'anxious realiz', 'anxious receiv']\n"
     ]
    }
   ],
   "source": [
    "features=tf_idf_vect.get_feature_names()#gets all feature names present\n",
    "print(\"some sample features(unique words in the corpus)\",features[100000:100010])#prints 10 features between 100000 to 100010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying top 25 features with there tf-idf\n",
    "def top_tfidf_feats(row,features,top_n=25):\n",
    "    topn_ids=np.argsort(row)[::-1][:top_n]\n",
    "    top_feats=[(features[i],row[i])for i in topn_ids]\n",
    "    df=pd.DataFrame(top_feats)\n",
    "    df.columns=['feature','tfidf']\n",
    "    return df\n",
    "\n",
    "top_tfidf=top_tfidf_feats(final_tf_idf[1,:].toarray()[0],features,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>read sendak</td>\n",
       "      <td>0.192657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>incorpor love</td>\n",
       "      <td>0.192657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flimsi take</td>\n",
       "      <td>0.192657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>version paperback</td>\n",
       "      <td>0.192657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>page open</td>\n",
       "      <td>0.192657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rosi movi</td>\n",
       "      <td>0.192657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>grew read</td>\n",
       "      <td>0.192657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>movi incorpor</td>\n",
       "      <td>0.192657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>paperback seem</td>\n",
       "      <td>0.192657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>keep page</td>\n",
       "      <td>0.192657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cover version</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sendak book</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>realli rosi</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>miss hard</td>\n",
       "      <td>0.182473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kind flimsi</td>\n",
       "      <td>0.176515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hard cover</td>\n",
       "      <td>0.174250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>watch realli</td>\n",
       "      <td>0.170558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>book watch</td>\n",
       "      <td>0.170558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sendak</td>\n",
       "      <td>0.166331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>howev miss</td>\n",
       "      <td>0.165155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>paperback</td>\n",
       "      <td>0.162104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hand keep</td>\n",
       "      <td>0.153882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>two hand</td>\n",
       "      <td>0.150189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rosi</td>\n",
       "      <td>0.148641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>seem kind</td>\n",
       "      <td>0.146910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature     tfidf\n",
       "0         read sendak  0.192657\n",
       "1       incorpor love  0.192657\n",
       "2         flimsi take  0.192657\n",
       "3   version paperback  0.192657\n",
       "4           page open  0.192657\n",
       "5           rosi movi  0.192657\n",
       "6           grew read  0.192657\n",
       "7       movi incorpor  0.192657\n",
       "8      paperback seem  0.192657\n",
       "9           keep page  0.192657\n",
       "10      cover version  0.186700\n",
       "11        sendak book  0.186700\n",
       "12        realli rosi  0.186700\n",
       "13          miss hard  0.182473\n",
       "14        kind flimsi  0.176515\n",
       "15         hard cover  0.174250\n",
       "16       watch realli  0.170558\n",
       "17         book watch  0.170558\n",
       "18             sendak  0.166331\n",
       "19         howev miss  0.165155\n",
       "20          paperback  0.162104\n",
       "21          hand keep  0.153882\n",
       "22           two hand  0.150189\n",
       "23               rosi  0.148641\n",
       "24          seem kind  0.146910"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf#displaying top 25 names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own Word2Vec model using your own text corpus\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in final['CleanedText'].values:\n",
    "    list_of_sent.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "witti littl book make son laugh loud recit car drive along alway sing refrain hes learn whale india droop love new word book introduc silli classic book will bet son still abl recit memori colleg\n",
      "*****************************************************************\n",
      "['witti', 'littl', 'book', 'make', 'son', 'laugh', 'loud', 'recit', 'car', 'drive', 'along', 'alway', 'sing', 'refrain', 'hes', 'learn', 'whale', 'india', 'droop', 'love', 'new', 'word', 'book', 'introduc', 'silli', 'classic', 'book', 'will', 'bet', 'son', 'still', 'abl', 'recit', 'memori', 'colleg']\n"
     ]
    }
   ],
   "source": [
    "print(final['CleanedText'].values[0])\n",
    "print(\"*****************************************************************\")\n",
    "print(list_of_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is Word2Vec converter\n",
    "w2v_model=Word2Vec(list_of_sent,min_count=5,size=50, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words that occured minimum 5 times  21761\n",
      "sample words  ['witti', 'littl', 'book', 'make', 'son', 'laugh', 'loud', 'recit', 'car', 'drive', 'along', 'alway', 'sing', 'refrain', 'hes', 'learn', 'whale', 'india', 'droop', 'love', 'new', 'word', 'introduc', 'silli', 'classic', 'will', 'bet', 'still', 'abl', 'memori', 'colleg', 'grew', 'read', 'sendak', 'watch', 'realli', 'rosi', 'movi', 'incorpor', 'howev', 'miss', 'hard', 'cover', 'version', 'paperback', 'seem', 'kind', 'flimsi', 'take', 'two']\n"
     ]
    }
   ],
   "source": [
    "w2v_words = list(w2v_model.wv.vocab)#it gives list of all the words we have\n",
    "#consider .wv as it is\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_words))\n",
    "print(\"sample words \", w2v_words[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delici', 0.8112898468971252),\n",
       " ('yummi', 0.7981356382369995),\n",
       " ('tastey', 0.7343237400054932),\n",
       " ('satisfi', 0.6929919123649597),\n",
       " ('nutriti', 0.6858402490615845),\n",
       " ('good', 0.6812770366668701),\n",
       " ('nice', 0.665293276309967),\n",
       " ('hearti', 0.6613863110542297),\n",
       " ('terrif', 0.6413846611976624),\n",
       " ('crunchi', 0.6332703232765198)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note:tasti is stemming word of tasty...\n",
    "w2v_model.wv.most_similar('tasti')#this prints all similar words to given word in decreasing order..and similarity level is measured between 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weird', 0.730994462966919),\n",
       " ('dislik', 0.6865944266319275),\n",
       " ('okay', 0.6859008073806763),\n",
       " ('resembl', 0.6665248870849609),\n",
       " ('except', 0.6475203633308411),\n",
       " ('gross', 0.6475184559822083),\n",
       " ('appeal', 0.6473760604858398),\n",
       " ('funki', 0.6422266364097595),\n",
       " ('prefer', 0.6403094530105591),\n",
       " ('hate', 0.6326681971549988)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg W2V, TFIDF-W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 364171/364171 [28:37<00:00, 211.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364171\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "#Avg W2V\n",
    "sent_vectors=[]\n",
    "for sent in tqdm(list_of_sent):\n",
    "    sent_vec=np.zeros(50)\n",
    "    cnt_words=0\n",
    "    for word in sent:\n",
    "        if word in w2v_words:\n",
    "            vec =w2v_model.wv[word]\n",
    "            sent_vec+=vec\n",
    "            cnt_words+=1\n",
    "    if cnt_words!=0:\n",
    "        sent_vec/=cnt_words#Avg W2V calculation\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=TfidfVectorizer()\n",
    "tf_idf_matrix=model.fit_transform(final['CleanedText'].values)\n",
    "#converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary=dict(zip(model.get_feature_names(),list(model.idf_)))#model.idf_ calculates idf of each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 364171/364171 [35:41<00:00, 170.05it/s]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF Weighted W2V\n",
    "tfidf_feat=model.get_feature_names()\n",
    "tfidf_sent_vectors=[]\n",
    "row=0\n",
    "for sent in tqdm(list_of_sent):\n",
    "    sent_vec=np.zeros(50)\n",
    "    weight_sum=0\n",
    "    for word in sent:\n",
    "        if word in w2v_words:\n",
    "            vec=w2v_model.wv[word]\n",
    "            #tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n",
    "            #to reduce the computation we have considered \n",
    "            #dictionary[word] = idf value of word in whole corpus\n",
    "            #sent.count(word) = tf values of word in this review\n",
    "            tf_idf=dictionary[word]*(sent.count(word)/len(sent))\n",
    "            tf_idf=dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec+=(vec*tf_idf)\n",
    "            weight_sum+=tf_idf\n",
    "    if weight_sum!=0:\n",
    "        sent_vec/=weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
